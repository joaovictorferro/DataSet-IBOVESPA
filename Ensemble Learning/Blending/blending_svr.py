# -*- coding: utf-8 -*-
"""Blending_SVR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    XXXX

# Imports
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import time
from sklearn.metrics import mean_squared_error
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_log_error
from sklearn.metrics import r2_score

from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor

df = pd.read_csv("XXXX")

"""# Blending DT

# Division Train and Test
"""

X = df.drop(['Close'],axis=1)
Y = df['Close']

# division Train and Test
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=23)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.3,
                                                                      random_state=23)

"""# Blending SVR"""

time_init = time.time()
# Define weak learners
weak_learners = [('dt', DecisionTreeRegressor(criterion= 'friedman_mse', max_depth = 4)),
                ('svr', SVR(C= 1, epsilon = 0.1, gamma = 'auto', kernel = 'rbf')),
                ('mlp', MLPRegressor(activation= 'identity', batch_size= 32, hidden_layer_sizes= (34), learning_rate = 'constant', solver= 'adam')),
                ]
# Final learner or meta model
final_learner =  SVR(C= 1, epsilon = 0.1, gamma = 'auto', kernel = 'rbf')

train_meta_model = None
test_meta_model = None

def train_level_0(clf):
  # Train with base x_train
  clf.fit(x_train, y_train)

  # Generate predictions for the holdout set (validation)
  # These predictions will build the input for the meta model
  val_predictions = clf.predict(x_val)

  # Generate predictions for original test set
  # These predictions will be used to test the meta model
  test_predictions = clf.predict(x_test)

  return val_predictions, test_predictions

def train_level_1(final_learner, train_meta_model, test_meta_model):
  # Train is carried out with final learner or meta model
  final_learner.fit(train_meta_model, y_val)

  # Getting train and test accuracies from meta_model
  y_pred = final_learner.predict(test_meta_model)

  mse_score = mean_squared_error(y_test , y_pred)

  evs_score = explained_variance_score(y_test, y_pred)

  mae_score = mean_absolute_error(y_test, y_pred)

  msle_score = mean_squared_log_error(y_test, y_pred)

  r2_s = r2_score(y_test, y_pred)

  time_final = time.time()

  print("MSE:", mse_score)
  print("EVS:", evs_score)
  print("MAE:", mae_score)
  print("MSLE:", msle_score)
  print("r2:", r2_s)
  print("Time:", time_final - time_init)

for clf_id, clf in weak_learners:

    # Predictions for each classifier based on k-fold
    val_predictions, test_predictions = train_level_0(clf)

    # Stack predictions which will form 
    # the input data for the data model
    if isinstance(train_meta_model, np.ndarray):
        train_meta_model = np.vstack((train_meta_model, val_predictions))
    else:
        train_meta_model = val_predictions

    # Stack predictions from test set
    # which will form test data for meta model
    if isinstance(test_meta_model, np.ndarray):
        test_meta_model = np.vstack((test_meta_model, test_predictions))
    else:
        test_meta_model = test_predictions

# Transpose train_meta_model
train_meta_model = train_meta_model.T

# Transpose test_meta_model
test_meta_model = test_meta_model.T

# Training level 1
train_level_1(final_learner, train_meta_model, test_meta_model)
